{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cSBFUe4FC52R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from joblib import Parallel, delayed\n",
        "import warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O3rSXGl8Cws-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth: int = 5, min_size: int = 10, random_features: bool = False):\n",
        "        # Hyperparameters (moved from fit → instance variables)\n",
        "        self.max_depth = max_depth\n",
        "        self.min_size = min_size\n",
        "        self.random_features = random_features\n",
        "\n",
        "        # Tree data (populated by fit)\n",
        "        self.dims = tuple()\n",
        "        self.boundaries = np.array([])\n",
        "        self.features = np.array([])\n",
        "        self.averages = np.array([])\n",
        "        self.samples = np.array([])\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        max_depth: int | None = None,\n",
        "        min_size: int | None = None,\n",
        "        random_features: bool | None = None,\n",
        "    ):\n",
        "        # Resolve hyperparameters (method args override instance variables)\n",
        "        max_depth = self.max_depth if max_depth is None else max_depth\n",
        "        min_size = self.min_size if min_size is None else min_size\n",
        "        random_features = self.random_features if random_features is None else random_features\n",
        "\n",
        "        # Save back (so subsequent calls use the latest values)\n",
        "        self.max_depth = max_depth\n",
        "        self.min_size = min_size\n",
        "        self.random_features = random_features\n",
        "\n",
        "        # Initialization\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        # Input validation\n",
        "        if np.isnan(X).any():\n",
        "            warnings.warn(\"NaN values detected and removed.\", RuntimeWarning)\n",
        "            valid_mask = ~np.isnan(X).any(axis=1)\n",
        "            X = X[valid_mask]\n",
        "            y = y[valid_mask]\n",
        "\n",
        "        X = X.astype(np.float32)\n",
        "        y = y.astype(np.float32)\n",
        "\n",
        "        if not np.issubdtype(X.dtype, np.number):\n",
        "            raise ValueError(\n",
        "                f\"X contains non-numeric data (dtype: {X.dtype}). \"\n",
        "                \"All features must be numeric. Please encode categorical features before fitting.\"\n",
        "            )\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        self.dims = (n_samples, n_features)\n",
        "\n",
        "        max_nodes = 2 ** (max_depth + 1) - 1\n",
        "        self.features = np.full(max_nodes, -1, dtype=np.int16)\n",
        "        self.boundaries = np.zeros(max_nodes, dtype=np.float32)\n",
        "        self.averages = np.zeros(max_nodes, dtype=np.float32)\n",
        "        self.samples = np.zeros(max_nodes, dtype=np.uint16)\n",
        "        self.samples[0] = n_samples\n",
        "\n",
        "        node_masks = np.zeros((max_nodes, n_samples), dtype=bool)\n",
        "        node_masks[0, :] = True\n",
        "\n",
        "        if random_features:\n",
        "            n_random_features = max(1, int(np.sqrt(n_features)))\n",
        "\n",
        "        # Loop over levels\n",
        "        depth = 0\n",
        "        while depth < max_depth:\n",
        "            n_leaves = 2 ** depth\n",
        "            node_idx_start = n_leaves - 1\n",
        "            node_idx_end = 2 * node_idx_start\n",
        "\n",
        "            no_splits = True\n",
        "\n",
        "            # Loop over (current) leaf nodes\n",
        "            for node_idx in range(node_idx_start, node_idx_end + 1):\n",
        "                mask = node_masks[node_idx]\n",
        "\n",
        "                if not mask.any():\n",
        "                    continue\n",
        "\n",
        "                best_split_loss = np.inf\n",
        "                best_split = None\n",
        "\n",
        "                # Select features to consider for split (for random forest)\n",
        "                if random_features:\n",
        "                    feature_indices = np.random.choice(\n",
        "                        n_features, size=n_random_features, replace=False\n",
        "                    )\n",
        "                else:\n",
        "                    feature_indices = np.arange(n_features)\n",
        "\n",
        "                X_node = X[mask]\n",
        "                y_node = y[mask]\n",
        "                node_size = mask.sum()\n",
        "\n",
        "                # Loop over features\n",
        "                for col_idx in feature_indices:\n",
        "                    feature_vals = X_node[:, col_idx]\n",
        "\n",
        "                    unique_values = np.unique(feature_vals)\n",
        "\n",
        "                    if len(unique_values) <= 1:\n",
        "                        continue\n",
        "\n",
        "                    # Find possible splits\n",
        "                    splits = (unique_values[:-1] + unique_values[1:]) / 2\n",
        "                    left_masks = feature_vals[:, None] <= splits[None, :]\n",
        "                    left_sizes = left_masks.sum(axis=0)\n",
        "                    right_sizes = node_size - left_sizes\n",
        "\n",
        "                    valid_splits = (left_sizes >= min_size) & (right_sizes >= min_size)\n",
        "\n",
        "                    if not valid_splits.any():\n",
        "                        continue\n",
        "\n",
        "                    valid_indices = np.where(valid_splits)[0]\n",
        "\n",
        "                    # Loop over possible (valid) splits\n",
        "                    for idx in valid_indices:\n",
        "                        left_mask_local = left_masks[:, idx]\n",
        "\n",
        "                        y_left = y_node[left_mask_local]\n",
        "                        y_right = y_node[~left_mask_local]\n",
        "\n",
        "                        left_size = len(y_left)\n",
        "                        right_size = len(y_right)\n",
        "                        parent_size = node_size\n",
        "\n",
        "                        # Mean squared error loss\n",
        "                        mse_left = np.var(y_left)\n",
        "                        mse_right = np.var(y_right)\n",
        "\n",
        "                        # Weighted MSE loss (by size of child node / size of parent node)\n",
        "                        loss = (\n",
        "                            left_size / parent_size * mse_left\n",
        "                            + right_size / parent_size * mse_right\n",
        "                        )\n",
        "\n",
        "                        if loss < best_split_loss:\n",
        "                            best_split_loss = loss\n",
        "                            best_split = (col_idx, splits[idx], left_mask_local)\n",
        "\n",
        "                # Perform split if found\n",
        "                if best_split is not None:\n",
        "                    feature, boundary, left_mask_local = best_split\n",
        "\n",
        "                    self.features[node_idx] = feature\n",
        "                    self.boundaries[node_idx] = boundary\n",
        "\n",
        "                    global_left_mask = mask.copy()\n",
        "                    global_left_mask[mask] = left_mask_local\n",
        "\n",
        "                    global_right_mask = mask.copy()\n",
        "                    global_right_mask[mask] = ~left_mask_local\n",
        "\n",
        "                    left_child = 2 * node_idx + 1\n",
        "                    right_child = 2 * node_idx + 2\n",
        "\n",
        "                    node_masks[left_child] = global_left_mask\n",
        "                    node_masks[right_child] = global_right_mask\n",
        "                    self.samples[left_child] = global_left_mask.sum()\n",
        "                    self.samples[right_child] = global_right_mask.sum()\n",
        "\n",
        "                    no_splits = False\n",
        "                else:\n",
        "                    self.features[node_idx] = -1\n",
        "                    self.averages[node_idx] = np.mean(y_node)\n",
        "                    node_masks[2 * node_idx + 1] = False\n",
        "                    node_masks[2 * node_idx + 2] = False\n",
        "\n",
        "            depth += 1\n",
        "\n",
        "            # Set node averages for leaf nodes at max depth\n",
        "            if no_splits or depth == max_depth:\n",
        "                for node_idx in range(node_idx_start, node_idx_end + 1):\n",
        "                    if node_masks[node_idx].any():\n",
        "                        self.features[node_idx] = -1\n",
        "                        self.averages[node_idx] = np.mean(y[node_masks[node_idx]])\n",
        "            if no_splits:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "\n",
        "        predictions = np.zeros(len(X))\n",
        "\n",
        "        for i, sample in enumerate(X):\n",
        "            node_idx = 0\n",
        "\n",
        "            while self.features[node_idx] != -1:\n",
        "                feature = self.features[node_idx]\n",
        "                boundary = self.boundaries[node_idx]\n",
        "\n",
        "                if 2 * node_idx + 1 >= len(self.features):\n",
        "                    break\n",
        "                elif sample[feature] <= boundary:\n",
        "                    node_idx = 2 * node_idx + 1\n",
        "                else:\n",
        "                    node_idx = 2 * node_idx + 2\n",
        "\n",
        "            predictions[i] = self.averages[node_idx]\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def data(self):\n",
        "        return {\n",
        "            \"dims\": self.dims,\n",
        "            \"features\": self.features,\n",
        "            \"boundaries\": self.boundaries,\n",
        "            \"averages\": self.averages,\n",
        "        }\n",
        "\n",
        "    def print_rules(self, feature_names=None):\n",
        "        if feature_names is None:\n",
        "            feature_names = [f\"feature_{i}\" for i in range(self.dims[1])]\n",
        "\n",
        "        stack = [(0, 0, True)]\n",
        "        while len(stack) > 0:\n",
        "            node_idx, depth, left = stack.pop()\n",
        "            print(\"|   \" * depth, end=\"\")\n",
        "            print(\"|--- \", end=\"\")\n",
        "            if self.features[node_idx] == -1:\n",
        "                print(\n",
        "                    \"class: \",\n",
        "                    self.averages[node_idx],\n",
        "                    f\"(samples: {self.samples[node_idx]})\",\n",
        "                    end=\"\",\n",
        "                )\n",
        "            else:\n",
        "                if left:\n",
        "                    print(\n",
        "                        f\"{feature_names[self.features[node_idx]]} <= \"\n",
        "                        f\"{self.boundaries[node_idx]}\",\n",
        "                        end=\"\",\n",
        "                    )\n",
        "                    stack.append((2 * node_idx + 2, depth + 1, False))\n",
        "                    stack.append((2 * node_idx + 2, depth + 1, True))\n",
        "                    stack.append((node_idx, depth, False))\n",
        "                    stack.append((2 * node_idx + 1, depth + 1, False))\n",
        "                    stack.append((2 * node_idx + 1, depth + 1, True))\n",
        "                else:\n",
        "                    print(\n",
        "                        f\"{feature_names[self.features[node_idx]]} > \"\n",
        "                        f\"{self.boundaries[node_idx]}\",\n",
        "                        end=\"\",\n",
        "                    )\n",
        "\n",
        "            print(\"\\n\", end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pATitjrtC3ZS"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RandomForestRegressor:\n",
        "    def __init__(self, n_trees:int=100, max_depth:int=5, min_size:int=1, n_jobs:int=-1):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_size = min_size\n",
        "        self.n_jobs = n_jobs\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        #Input validation\n",
        "        if np.isnan(X).any():\n",
        "            warnings.warn(\"NaN values detected and removed.\", RuntimeWarning)\n",
        "            valid_mask = ~np.isnan(X).any(axis=1)\n",
        "            X = X[valid_mask]\n",
        "            y = y[valid_mask]\n",
        "\n",
        "        X=X.astype(np.float32)\n",
        "        y=y.astype(np.float32)\n",
        "\n",
        "        if not np.issubdtype(X.dtype, np.number):\n",
        "            raise ValueError(\n",
        "                f\"X contains non-numeric data (dtype: {X.dtype}). \"\n",
        "                \"All features must be numeric. Please encode categorical features before fitting.\"\n",
        "            )\n",
        "\n",
        "\n",
        "        self.trees = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(self._fit_single_tree)(X, y,i) for i in range(self.n_trees)\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def _fit_single_tree(self, X, y,seed):\n",
        "        tree = DecisionTreeRegressor()\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        bootstrap_indices = np.random.choice(len(X), size= len(X), replace=True)\n",
        "        tree.fit(X[bootstrap_indices], y[bootstrap_indices], max_depth=self.max_depth, min_size=self.min_size, random_features=True)\n",
        "        return tree\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array(Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(tree.predict)(X) for tree in self.trees\n",
        "        ))\n",
        "        return np.mean(predictions, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sgIPTzREvGX",
        "outputId": "3c20ce76-e706-4c6a-b804-4862f3d457e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "1.1 Custom RandomForestRegressor (LT-FS-ID Dataset)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Nested CV Summary =====\n",
            "Mean outer RMSE: 17.6786\n",
            "Std outer RMSE:  2.7346\n",
            "Mean outer R²:   0.9225\n",
            "Std outer R²:    0.0166\n",
            "\n",
            "===== Training Time =====\n",
            "Mean inner grid-search time (sec): 115.382\n",
            "Mean outer fit time (sec):         2.405\n",
            "\n",
            "===== Best Model =====\n",
            "Best outer RMSE: 14.749608465216372\n",
            "Best hyperparameters:\n",
            "  n_trees: 100\n",
            "  max_depth: 10\n",
            "  min_size: 5\n",
            "\n",
            "===== Per-Fold Results =====\n",
            "   fold  outer_rmse  outer_r2  best_inner_rmse  best_inner_r2  inner_search_time_sec  best_inner_fit_time_sec  outer_fit_time_sec  n_trees  max_depth  min_size\n",
            "0     2   14.749608  0.938249        19.169718       0.915862             113.688404                11.097999            2.221452      100         10         5\n",
            "1     4   15.106599  0.921781        19.410076       0.908443             114.169494                11.327356            2.231670      100         10         5\n",
            "2     5   18.062803  0.929742        19.796545       0.901435             114.195231                11.179212            2.685077      100         10         5\n",
            "3     3   19.376553  0.927943        18.390752       0.909717             115.272439                11.379482            2.678215      100         10         5\n",
            "4     1   21.097208  0.894733        19.597438       0.902648             119.586897                11.153864            2.210048      100         10         5\n",
            "\n",
            "\n",
            "============================================================\n",
            "1.2 Sklearn RandomForestRegressor (LT-FS-ID Dataset)\n",
            "============================================================\n",
            "===== Nested CV Summary =====\n",
            "Mean outer RMSE: 15.0343\n",
            "Std outer RMSE:  2.8452\n",
            "Mean outer R²:   0.9439\n",
            "Std outer R²:    0.0135\n",
            "\n",
            "===== Training Time =====\n",
            "Mean inner grid-search time (sec): 11.255\n",
            "Mean outer fit time (sec):         0.111\n",
            "\n",
            "===== Best Model =====\n",
            "Best outer RMSE: 11.094752216027832\n",
            "Best hyperparameters:\n",
            "  max_depth: 7\n",
            "  min_samples_leaf: 5\n",
            "  n_estimators: 50\n",
            "\n",
            "===== Per-Fold Results =====\n",
            "   fold  outer_rmse  outer_r2  best_inner_score  inner_search_time_sec  outer_fit_time_sec  max_depth  min_samples_leaf  n_estimators\n",
            "0     2   11.094752  0.965061         17.627963               9.885020            0.112170          7                 5            50\n",
            "1     4   13.692573  0.935739         16.409993              11.161720            0.063663          7                 5            50\n",
            "2     5   15.236462  0.950009         17.856505              11.230264            0.130122          7                 5           100\n",
            "3     1   16.544074  0.935267         16.350547              13.205328            0.121805          7                 5           100\n",
            "4     3   18.603719  0.933577         16.524893              10.793165            0.126256          7                 5           100\n",
            "\n",
            "\n",
            "============================================================\n",
            "COMPARISON: Custom vs Sklearn RandomForest\n",
            "============================================================\n",
            "Custom RF   - Mean RMSE: 17.6786 (±2.7346)\n",
            "Sklearn RF  - Mean RMSE: 15.0343 (±2.8452)\n",
            "\n",
            "Custom RF   - Mean R²: 0.9225 (±0.0166)\n",
            "Sklearn RF  - Mean R²: 0.9439 (±0.0135)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from itertools import product\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import RandomForestRegressor as SkRandomForestRegressor\n",
        "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import root_mean_squared_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def nested_grid_search_custom_rf(\n",
        "    X, y, grid_params,\n",
        "    outer_splits=5, inner_splits=5,\n",
        "    random_state=42\n",
        "):\n",
        "    \"\"\"Nested CV with custom RandomForestRegressor\"\"\"\n",
        "    outer_cv = KFold(n_splits=outer_splits, shuffle=True, random_state=random_state)\n",
        "    inner_cv = KFold(n_splits=inner_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    keys = list(grid_params.keys())\n",
        "    combos = [dict(zip(keys, vals)) for vals in product(*(grid_params[k] for k in keys))]\n",
        "\n",
        "    outer_results = []\n",
        "    best_outer_model = None\n",
        "    best_outer_score = np.inf\n",
        "    best_outer_params = None\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X), start=1):\n",
        "        X_train = X.iloc[train_idx].to_numpy() if hasattr(X, 'iloc') else X[train_idx]\n",
        "        y_train = y.iloc[train_idx].to_numpy() if hasattr(y, 'iloc') else y[train_idx]\n",
        "        X_test = X.iloc[test_idx].to_numpy() if hasattr(X, 'iloc') else X[test_idx]\n",
        "        y_test = y.iloc[test_idx].to_numpy() if hasattr(y, 'iloc') else y[test_idx]\n",
        "\n",
        "        # Inner grid search\n",
        "        best_inner_params = None\n",
        "        best_inner_score = np.inf\n",
        "        best_inner_r2 = None\n",
        "        inner_total_time = 0.0\n",
        "\n",
        "        for params in tqdm(combos, desc=f\"[{fold}] inner grid\", leave=False):\n",
        "            inner_rmse_scores = []\n",
        "            inner_r2_scores = []\n",
        "            param_time = 0.0\n",
        "\n",
        "            for tr_i, va_i in inner_cv.split(X_train):\n",
        "                X_tr = X_train[tr_i]\n",
        "                y_tr = y_train[tr_i]\n",
        "                X_va = X_train[va_i]\n",
        "                y_va = y_train[va_i]\n",
        "\n",
        "                model = RandomForestRegressor(**params)\n",
        "\n",
        "                start = time.perf_counter()\n",
        "                model.fit(X_tr, y_tr)\n",
        "                param_time += time.perf_counter() - start\n",
        "\n",
        "                pred = model.predict(X_va)\n",
        "\n",
        "                inner_rmse_scores.append(root_mean_squared_error(y_va, pred))\n",
        "                inner_r2_scores.append(r2_score(y_va, pred))\n",
        "\n",
        "            mean_inner_rmse = float(np.mean(inner_rmse_scores))\n",
        "            mean_inner_r2 = float(np.mean(inner_r2_scores))\n",
        "            inner_total_time += param_time\n",
        "\n",
        "            if mean_inner_rmse < best_inner_score:\n",
        "                best_inner_score = mean_inner_rmse\n",
        "                best_inner_params = params\n",
        "                best_inner_r2 = mean_inner_r2\n",
        "                best_inner_time = param_time\n",
        "\n",
        "        # Refit on outer train\n",
        "        final_model = RandomForestRegressor(**best_inner_params)\n",
        "\n",
        "        start = time.perf_counter()\n",
        "        final_model.fit(X_train, y_train)\n",
        "        outer_train_time = time.perf_counter() - start\n",
        "\n",
        "        test_pred = final_model.predict(X_test)\n",
        "        outer_rmse = float(root_mean_squared_error(y_test, test_pred))\n",
        "        outer_r2 = float(r2_score(y_test, test_pred))\n",
        "\n",
        "        outer_results.append({\n",
        "            \"fold\": fold,\n",
        "            \"outer_rmse\": outer_rmse,\n",
        "            \"outer_r2\": outer_r2,\n",
        "            \"best_inner_rmse\": best_inner_score,\n",
        "            \"best_inner_r2\": best_inner_r2,\n",
        "            \"inner_search_time_sec\": inner_total_time,\n",
        "            \"best_inner_fit_time_sec\": best_inner_time,\n",
        "            \"outer_fit_time_sec\": outer_train_time,\n",
        "            **best_inner_params\n",
        "        })\n",
        "\n",
        "        if outer_rmse < best_outer_score:\n",
        "            best_outer_score = outer_rmse\n",
        "            best_outer_model = final_model\n",
        "            best_outer_params = best_inner_params\n",
        "\n",
        "    results_df = pd.DataFrame(outer_results).sort_values(\"outer_rmse\").reset_index(drop=True)\n",
        "\n",
        "    summary = {\n",
        "        \"mean_outer_rmse\": float(results_df[\"outer_rmse\"].mean()),\n",
        "        \"std_outer_rmse\": float(results_df[\"outer_rmse\"].std(ddof=1)),\n",
        "        \"mean_outer_r2\": float(results_df[\"outer_r2\"].mean()),\n",
        "        \"std_outer_r2\": float(results_df[\"outer_r2\"].std(ddof=1)),\n",
        "        \"mean_outer_fit_time_sec\": float(results_df[\"outer_fit_time_sec\"].mean()),\n",
        "        \"mean_inner_search_time_sec\": float(results_df[\"inner_search_time_sec\"].mean()),\n",
        "        \"best_outer_rmse\": best_outer_score,\n",
        "        \"best_outer_params\": best_outer_params,\n",
        "        \"best_outer_model\": best_outer_model,\n",
        "        \"results\": results_df\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "\n",
        "def nested_grid_search_sklearn_rf(\n",
        "    X, y,\n",
        "    estimator,\n",
        "    param_grid,\n",
        "    *,\n",
        "    outer_splits=5,\n",
        "    inner_splits=5,\n",
        "    random_state=42,\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        "    n_jobs=-1,\n",
        "    verbose=0,\n",
        "):\n",
        "    \"\"\"Nested CV with sklearn RandomForestRegressor\"\"\"\n",
        "    outer_cv = KFold(n_splits=outer_splits, shuffle=True, random_state=random_state)\n",
        "    inner_cv = KFold(n_splits=inner_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    X_np = X.to_numpy() if hasattr(X, \"to_numpy\") else np.asarray(X)\n",
        "    y_np = y.to_numpy() if hasattr(y, \"to_numpy\") else np.asarray(y)\n",
        "\n",
        "    outer_results = []\n",
        "    best_outer_model = None\n",
        "    best_outer_primary = np.inf\n",
        "    best_outer_params = None\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X_np), start=1):\n",
        "        X_train, y_train = X_np[train_idx], y_np[train_idx]\n",
        "        X_test, y_test = X_np[test_idx], y_np[test_idx]\n",
        "\n",
        "        gs = GridSearchCV(\n",
        "            estimator=clone(estimator),\n",
        "            param_grid=param_grid,\n",
        "            cv=inner_cv,\n",
        "            scoring=scoring,\n",
        "            n_jobs=n_jobs,\n",
        "            refit=True,\n",
        "            verbose=verbose,\n",
        "            return_train_score=False,\n",
        "        )\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        gs.fit(X_train, y_train)\n",
        "        inner_search_time = time.perf_counter() - t0\n",
        "\n",
        "        best_params = gs.best_params_\n",
        "        best_est = clone(estimator).set_params(**best_params)\n",
        "\n",
        "        t1 = time.perf_counter()\n",
        "        best_est.fit(X_train, y_train)\n",
        "        outer_fit_time = time.perf_counter() - t1\n",
        "\n",
        "        y_pred = best_est.predict(X_test)\n",
        "        outer_rmse = float(root_mean_squared_error(y_test, y_pred))\n",
        "        outer_r2 = float(r2_score(y_test, y_pred))\n",
        "\n",
        "        best_inner_score = gs.best_score_\n",
        "        if scoring.startswith(\"neg_\"):\n",
        "            best_inner_score = float(-best_inner_score)\n",
        "        else:\n",
        "            best_inner_score = float(best_inner_score)\n",
        "\n",
        "        outer_results.append({\n",
        "            \"fold\": fold,\n",
        "            \"outer_rmse\": outer_rmse,\n",
        "            \"outer_r2\": outer_r2,\n",
        "            \"best_inner_score\": best_inner_score,\n",
        "            \"inner_search_time_sec\": float(inner_search_time),\n",
        "            \"outer_fit_time_sec\": float(outer_fit_time),\n",
        "            **best_params\n",
        "        })\n",
        "\n",
        "        if outer_rmse < best_outer_primary:\n",
        "            best_outer_primary = outer_rmse\n",
        "            best_outer_model = best_est\n",
        "            best_outer_params = best_params\n",
        "\n",
        "    results_df = pd.DataFrame(outer_results).sort_values(\"outer_rmse\").reset_index(drop=True)\n",
        "\n",
        "    summary = {\n",
        "        \"mean_outer_rmse\": float(results_df[\"outer_rmse\"].mean()),\n",
        "        \"std_outer_rmse\": float(results_df[\"outer_rmse\"].std(ddof=1)),\n",
        "        \"mean_outer_r2\": float(results_df[\"outer_r2\"].mean()),\n",
        "        \"std_outer_r2\": float(results_df[\"outer_r2\"].std(ddof=1)),\n",
        "        \"mean_inner_search_time_sec\": float(results_df[\"inner_search_time_sec\"].mean()),\n",
        "        \"mean_outer_fit_time_sec\": float(results_df[\"outer_fit_time_sec\"].mean()),\n",
        "        \"best_outer_rmse\": float(best_outer_primary),\n",
        "        \"best_outer_params\": best_outer_params,\n",
        "        \"best_outer_model\": best_outer_model,\n",
        "        \"results\": results_df,\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "\n",
        "def report_nested_grid_search(summary: dict):\n",
        "    \"\"\"Print nested CV results\"\"\"\n",
        "    print(\"===== Nested CV Summary =====\")\n",
        "    print(f\"Mean outer RMSE: {summary['mean_outer_rmse']:.4f}\")\n",
        "    print(f\"Std outer RMSE:  {summary['std_outer_rmse']:.4f}\")\n",
        "    print(f\"Mean outer R²:   {summary['mean_outer_r2']:.4f}\")\n",
        "    print(f\"Std outer R²:    {summary['std_outer_r2']:.4f}\")\n",
        "    print()\n",
        "\n",
        "    print(\"===== Training Time =====\")\n",
        "    print(f\"Mean inner grid-search time (sec): {summary['mean_inner_search_time_sec']:.3f}\")\n",
        "    print(f\"Mean outer fit time (sec):         {summary['mean_outer_fit_time_sec']:.3f}\")\n",
        "    print()\n",
        "\n",
        "    print(\"===== Best Model =====\")\n",
        "    print(\"Best outer RMSE:\", summary[\"best_outer_rmse\"])\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for k, v in summary[\"best_outer_params\"].items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "    print(\"\\n===== Per-Fold Results =====\")\n",
        "    print(summary[\"results\"].to_string())\n",
        "    print()\n",
        "\n",
        "\n",
        "# ==================== EXAMPLE USAGE ====================\n",
        "# Load your data (LT-FS-ID Dataset)\n",
        "if __name__ == \"__main__\":\n",
        "    df1 = pd.read_csv(\"/content/lt_fs_id.csv\")\n",
        "    X = df1.drop(columns=[\"Number of Barriers\"])\n",
        "    y = df1[\"Number of Barriers\"]\n",
        "\n",
        "    # ========== 1.1 Custom RandomForestRegressor ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"1.1 Custom RandomForestRegressor (LT-FS-ID Dataset)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    grid_params_rf_custom = {\n",
        "        \"n_trees\": [10, 50, 100],\n",
        "        \"max_depth\": [5, 7, 10],\n",
        "        \"min_size\": [5, 10, 20],\n",
        "    }\n",
        "\n",
        "    summary_rf_custom = nested_grid_search_custom_rf(\n",
        "        X=X, y=y, grid_params=grid_params_rf_custom,\n",
        "        inner_splits=5, outer_splits=5\n",
        "    )\n",
        "    report_nested_grid_search(summary_rf_custom)\n",
        "\n",
        "    # ========== 1.2 Sklearn RandomForestRegressor ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"1.2 Sklearn RandomForestRegressor (LT-FS-ID Dataset)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    grid_params_rf_sklearn = {\n",
        "        \"n_estimators\": [10, 50, 100],\n",
        "        \"max_depth\": [5, 7, 10],\n",
        "        \"min_samples_leaf\": [5, 10, 20],\n",
        "    }\n",
        "\n",
        "    summary_rf_sklearn = nested_grid_search_sklearn_rf(\n",
        "        X=X, y=y,\n",
        "        estimator=SkRandomForestRegressor(random_state=42),\n",
        "        param_grid=grid_params_rf_sklearn,\n",
        "        inner_splits=5, outer_splits=5\n",
        "    )\n",
        "    report_nested_grid_search(summary_rf_sklearn)\n",
        "\n",
        "    # ========== Comparison ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COMPARISON: Custom vs Sklearn RandomForest\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Custom RF   - Mean RMSE: {summary_rf_custom['mean_outer_rmse']:.4f} \"\n",
        "          f\"(\\u00b1{summary_rf_custom['std_outer_rmse']:.4f})\")\n",
        "    print(f\"Sklearn RF  - Mean RMSE: {summary_rf_sklearn['mean_outer_rmse']:.4f} \"\n",
        "          f\"(\\u00b1{summary_rf_sklearn['std_outer_rmse']:.4f})\")\n",
        "    print(f\"\\nCustom RF   - Mean R²: {summary_rf_custom['mean_outer_r2']:.4f} \"\n",
        "          f\"(\\u00b1{summary_rf_custom['std_outer_r2']:.4f})\")\n",
        "    print(f\"Sklearn RF  - Mean R²: {summary_rf_sklearn['mean_outer_r2']:.4f} \"\n",
        "          f\"(\\u00b1{summary_rf_sklearn['std_outer_r2']:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U6O4QaYTt_1",
        "outputId": "0b9e753b-7ed5-4697-8df3-c8253013f415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DATASET 2: PADDY YIELD PREDICTION\n",
            "======================================================================\n",
            "Encoded feature shape: (2789, 71)\n",
            "\n",
            "[2.1] Custom RandomForestRegressor\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Nested CV Summary =====\n",
            "Mean outer RMSE: 815.1376\n",
            "Std outer RMSE:  35.8105\n",
            "Mean outer R²:   0.9921\n",
            "Std outer R²:    0.0009\n",
            "\n",
            "===== Training Time =====\n",
            "Mean inner grid-search time (sec): 552.807\n",
            "Mean outer fit time (sec):         12.471\n",
            "\n",
            "===== Best Model =====\n",
            "Best outer RMSE: 752.4573834331629\n",
            "Best hyperparameters:\n",
            "  n_trees: 100\n",
            "  max_depth: 7\n",
            "  min_size: 5\n",
            "\n",
            "===== Per-Fold Results =====\n",
            "   fold  outer_rmse  outer_r2  best_inner_rmse  best_inner_r2  inner_search_time_sec  best_inner_fit_time_sec  outer_fit_time_sec  n_trees  max_depth  min_size\n",
            "0     5  752.457383  0.993626       838.964270       0.991550             553.946307                42.001270            8.866013      100          7         5\n",
            "1     2  818.495065  0.991567       820.071009       0.992094             552.486987                63.005545           13.401470      100         10         5\n",
            "2     3  832.103104  0.991905       815.213477       0.992095             554.167317                65.178395           13.848520      100         10         5\n",
            "3     1  834.800095  0.991408       811.567147       0.992235             548.936672                64.460101           13.497920      100         10         5\n",
            "4     4  837.832162  0.992001       809.074587       0.992151             554.497859                65.424341           12.740462      100         10         5\n",
            "\n",
            "\n",
            "[2.2] Sklearn RandomForestRegressor\n",
            "----------------------------------------------------------------------\n",
            "===== Nested CV Summary =====\n",
            "Mean outer RMSE: 811.4513\n",
            "Std outer RMSE:  30.7459\n",
            "Mean outer R²:   0.9922\n",
            "Std outer R²:    0.0008\n",
            "\n",
            "===== Training Time =====\n",
            "Mean inner grid-search time (sec): 48.705\n",
            "Mean outer fit time (sec):         0.233\n",
            "\n",
            "===== Best Model =====\n",
            "Best outer RMSE: 757.0415055133684\n",
            "Best hyperparameters:\n",
            "  max_depth: 5\n",
            "  min_samples_leaf: 20\n",
            "  n_estimators: 10\n",
            "\n",
            "===== Per-Fold Results =====\n",
            "   fold  outer_rmse  outer_r2  best_inner_score  inner_search_time_sec  outer_fit_time_sec  max_depth  min_samples_leaf  n_estimators\n",
            "0     5  757.041506  0.993548        823.977445              49.734751            0.083853          5                20            10\n",
            "1     2  820.071624  0.991535        805.507212              48.721546            0.576914          5                20            50\n",
            "2     3  822.879616  0.992083        804.999386              49.378954            0.364020          5                20            50\n",
            "3     1  825.056881  0.991607        807.052248              47.965755            0.071242          5                10            10\n",
            "4     4  832.206771  0.992108        801.500424              47.723128            0.069323          5                20            10\n",
            "\n",
            "\n",
            "[COMPARISON] Custom vs Sklearn RandomForest (Paddy)\n",
            "----------------------------------------------------------------------\n",
            "Custom RF   - Mean RMSE: 815.1376 (±35.8105)\n",
            "Sklearn RF  - Mean RMSE: 811.4513 (±30.7459)\n",
            "\n",
            "Custom RF   - Mean R²: 0.9921 (±0.0009)\n",
            "Sklearn RF  - Mean R²: 0.9922 (±0.0008)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ==================== DATASET 2: PADDY ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET 2: PADDY YIELD PREDICTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df2 = pd.read_csv(\"/content/paddydataset.csv\")\n",
        "X2 = df2.drop(columns=[\"Paddy yield(in Kg)\"])\n",
        "y2 = df2[\"Paddy yield(in Kg)\"]\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = X2.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "numeric_cols = X2.select_dtypes(exclude=[\"object\", \"category\"]).columns\n",
        "\n",
        "preprocessor2 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
        "        (\"num\", \"passthrough\", numeric_cols),\n",
        "    ]\n",
        ")\n",
        "preprocessor2.set_output(transform=\"pandas\")\n",
        "X2_enc = preprocessor2.fit_transform(X2)\n",
        "\n",
        "print(f\"Encoded feature shape: {X2_enc.shape}\")\n",
        "\n",
        "# 2.1 Custom RandomForestRegressor\n",
        "print(\"\\n[2.1] Custom RandomForestRegressor\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "grid_params_rf_custom_2 = {\n",
        "    \"n_trees\": [10, 50, 100],\n",
        "    \"max_depth\": [5, 7, 10],\n",
        "    \"min_size\": [5, 10, 20],\n",
        "}\n",
        "\n",
        "summary_rf_custom_2 = nested_grid_search_custom_rf(\n",
        "    X=X2_enc, y=y2, grid_params=grid_params_rf_custom_2,\n",
        "    inner_splits=5, outer_splits=5\n",
        ")\n",
        "report_nested_grid_search(summary_rf_custom_2)\n",
        "\n",
        "# 2.2 Sklearn RandomForestRegressor\n",
        "print(\"\\n[2.2] Sklearn RandomForestRegressor\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "grid_params_rf_sklearn_2 = {\n",
        "    \"n_estimators\": [10, 50, 100],\n",
        "    \"max_depth\": [5, 7, 10],\n",
        "    \"min_samples_leaf\": [5, 10, 20],\n",
        "}\n",
        "\n",
        "summary_rf_sklearn_2 = nested_grid_search_sklearn_rf(\n",
        "    X=X2_enc, y=y2,\n",
        "    estimator=SkRandomForestRegressor(random_state=42),\n",
        "    param_grid=grid_params_rf_sklearn_2,\n",
        "    inner_splits=5, outer_splits=5\n",
        ")\n",
        "report_nested_grid_search(summary_rf_sklearn_2)\n",
        "\n",
        "# Comparison\n",
        "print(\"\\n[COMPARISON] Custom vs Sklearn RandomForest (Paddy)\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Custom RF   - Mean RMSE: {summary_rf_custom_2['mean_outer_rmse']:.4f} \"\n",
        "      f\"(±{summary_rf_custom_2['std_outer_rmse']:.4f})\")\n",
        "print(f\"Sklearn RF  - Mean RMSE: {summary_rf_sklearn_2['mean_outer_rmse']:.4f} \"\n",
        "      f\"(±{summary_rf_sklearn_2['std_outer_rmse']:.4f})\")\n",
        "print(f\"\\nCustom RF   - Mean R²: {summary_rf_custom_2['mean_outer_r2']:.4f} \"\n",
        "      f\"(±{summary_rf_custom_2['std_outer_r2']:.4f})\")\n",
        "print(f\"Sklearn RF  - Mean R²: {summary_rf_sklearn_2['mean_outer_r2']:.4f} \"\n",
        "      f\"(±{summary_rf_sklearn_2['std_outer_r2']:.4f})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== DATASET 3: STEEL INDUSTRY ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET 3: STEEL INDUSTRY ENERGY CONSUMPTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df3 = pd.read_csv(\"/content/Steel_industry_data.csv\")\n",
        "df3[\"date\"] = pd.to_datetime(df3[\"date\"], format=\"%d/%m/%Y %H:%M\")\n",
        "df3[\"month\"] = df3[\"date\"].dt.month\n",
        "df3[\"hour\"] = df3[\"date\"].dt.hour\n",
        "\n",
        "X3 = df3.drop(columns=[\"date\", \"Usage_kWh\", \"Load_Type\"])\n",
        "y3 = df3[\"Usage_kWh\"]\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = X3.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "numeric_cols = X3.select_dtypes(exclude=[\"object\", \"category\"]).columns\n",
        "\n",
        "preprocessor3 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
        "        (\"num\", \"passthrough\", numeric_cols),\n",
        "    ]\n",
        ")\n",
        "preprocessor3.set_output(transform=\"pandas\")\n",
        "X3_enc = preprocessor3.fit_transform(X3)\n",
        "\n",
        "print(f\"Encoded feature shape: {X3_enc.shape}\")\n",
        "\n",
        "# 3.1 Custom RandomForestRegressor\n",
        "print(\"\\n[3.1] Custom RandomForestRegressor\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "grid_params_rf_custom_3 = {\n",
        "    \"n_trees\": [10, 50, 100],\n",
        "    \"max_depth\": [5, 10, 15],\n",
        "    \"min_size\": [5, 10, 20],\n",
        "}\n",
        "\n",
        "summary_rf_custom_3 = nested_grid_search_custom_rf(\n",
        "    X=X3_enc, y=y3, grid_params=grid_params_rf_custom_3,\n",
        "    inner_splits=4, outer_splits=2\n",
        ")\n",
        "report_nested_grid_search(summary_rf_custom_3)\n",
        "\n",
        "# 3.2 Sklearn RandomForestRegressor\n",
        "print(\"\\n[3.2] Sklearn RandomForestRegressor\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "grid_params_rf_sklearn_3 = {\n",
        "    \"n_estimators\": [10, 50, 100],\n",
        "    \"max_depth\": [5, 10, 15],\n",
        "    \"min_samples_leaf\": [5, 10, 20],\n",
        "}\n",
        "\n",
        "summary_rf_sklearn_3 = nested_grid_search_sklearn_rf(\n",
        "    X=X3_enc, y=y3,\n",
        "    estimator=SkRandomForestRegressor(random_state=42),\n",
        "    param_grid=grid_params_rf_sklearn_3,\n",
        "    inner_splits=5, outer_splits=5\n",
        ")\n",
        "report_nested_grid_search(summary_rf_sklearn_3)\n",
        "\n",
        "# Comparison\n",
        "print(\"\\n[COMPARISON] Custom vs Sklearn RandomForest (Steel Industry)\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Custom RF   - Mean RMSE: {summary_rf_custom_3['mean_outer_rmse']:.4f} \"\n",
        "      f\"(±{summary_rf_custom_3['std_outer_rmse']:.4f})\")\n",
        "print(f\"Sklearn RF  - Mean RMSE: {summary_rf_sklearn_3['mean_outer_rmse']:.4f} \"\n",
        "      f\"(±{summary_rf_sklearn_3['std_outer_rmse']:.4f})\")\n",
        "print(f\"\\nCustom RF   - Mean R²: {summary_rf_custom_3['mean_outer_r2']:.4f} \"\n",
        "      f\"(±{summary_rf_custom_3['std_outer_r2']:.4f})\")\n",
        "print(f\"Sklearn RF  - Mean R²: {summary_rf_sklearn_3['mean_outer_r2']:.4f} \"\n",
        "      f\"(±{summary_rf_sklearn_3['std_outer_r2']:.4f})\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YTHoMvXepbJ3",
        "outputId": "dcba2361-3ce2-4654-fd28-83e6e0a5ebd4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "DATASET 3: STEEL INDUSTRY ENERGY CONSUMPTION\n",
            "======================================================================\n",
            "Encoded feature shape: (35040, 17)\n",
            "\n",
            "[3.1] Custom RandomForestRegressor\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Nested CV Summary =====\n",
            "Mean outer RMSE: 2.7247\n",
            "Std outer RMSE:  0.2723\n",
            "Mean outer R²:   0.9933\n",
            "Std outer R²:    0.0013\n",
            "\n",
            "===== Training Time =====\n",
            "Mean inner grid-search time (sec): 16134.056\n",
            "Mean outer fit time (sec):         466.535\n",
            "\n",
            "===== Best Model =====\n",
            "Best outer RMSE: 2.532104507352613\n",
            "Best hyperparameters:\n",
            "  n_trees: 50\n",
            "  max_depth: 15\n",
            "  min_size: 5\n",
            "\n",
            "===== Per-Fold Results =====\n",
            "   fold  outer_rmse  outer_r2  best_inner_rmse  best_inner_r2  inner_search_time_sec  best_inner_fit_time_sec  outer_fit_time_sec  n_trees  max_depth  min_size\n",
            "0     1    2.532105  0.994221         2.715745       0.993455           15885.058818               849.805519          307.448776       50         15         5\n",
            "1     2    2.917258  0.992452         2.771494       0.993053           16383.053797              1751.857646          625.620929      100         15         5\n",
            "\n",
            "\n",
            "[3.2] Sklearn RandomForestRegressor\n",
            "----------------------------------------------------------------------\n",
            "===== Nested CV Summary =====\n",
            "Mean outer RMSE: 1.2934\n",
            "Std outer RMSE:  0.1173\n",
            "Mean outer R²:   0.9985\n",
            "Std outer R²:    0.0003\n",
            "\n",
            "===== Training Time =====\n",
            "Mean inner grid-search time (sec): 450.574\n",
            "Mean outer fit time (sec):         13.109\n",
            "\n",
            "===== Best Model =====\n",
            "Best outer RMSE: 1.114698404142307\n",
            "Best hyperparameters:\n",
            "  max_depth: 15\n",
            "  min_samples_leaf: 5\n",
            "  n_estimators: 50\n",
            "\n",
            "===== Per-Fold Results =====\n",
            "   fold  outer_rmse  outer_r2  best_inner_score  inner_search_time_sec  outer_fit_time_sec  max_depth  min_samples_leaf  n_estimators\n",
            "0     4    1.114698  0.998906          1.375465             443.625109            7.206570         15                 5            50\n",
            "1     1    1.284843  0.998548          1.331215             453.337796           14.805638         15                 5           100\n",
            "2     5    1.291125  0.998536          1.422109             450.046858           14.513431         15                 5           100\n",
            "3     3    1.337746  0.998373          1.376425             451.958347           14.533405         15                 5           100\n",
            "4     2    1.438477  0.998084          1.332900             453.902133           14.485946         15                 5           100\n",
            "\n",
            "\n",
            "[COMPARISON] Custom vs Sklearn RandomForest (Steel Industry)\n",
            "----------------------------------------------------------------------\n",
            "Custom RF   - Mean RMSE: 2.7247 (±0.2723)\n",
            "Sklearn RF  - Mean RMSE: 1.2934 (±0.1173)\n",
            "\n",
            "Custom RF   - Mean R²: 0.9933 (±0.0013)\n",
            "Sklearn RF  - Mean R²: 0.9985 (±0.0003)\n",
            "\n",
            "======================================================================\n",
            "FINAL SUMMARY: ALL DATASETS\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'summary_rf_custom_1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1237466052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \"Paddy (Sklearn)\", \"Steel (Custom)\", \"Steel (Sklearn)\"],\n\u001b[1;32m     84\u001b[0m     \"Mean RMSE\": [\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0msummary_rf_custom_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_outer_rmse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0msummary_rf_sklearn_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_outer_rmse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0msummary_rf_custom_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_outer_rmse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'summary_rf_custom_1' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}